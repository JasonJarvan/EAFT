{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def eachFile(filepath):#read all the files\n",
    "    \"\"\"\n",
    "    get all the files in the dir\n",
    "    \"\"\"\n",
    "    pathDir =  os.listdir(filepath)\n",
    "    files = []\n",
    "    for allDir in pathDir:\n",
    "        child = os.path.join('%s%s' % (filepath, allDir))\n",
    "        if '.txt' in child:\n",
    "            files.append(child)\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_text(values, path):#read the text in the file\n",
    "    \"\"\"\n",
    "    get the text value in the file\n",
    "    \"\"\"\n",
    "    text = open(path, 'r', encoding='UTF-8')\n",
    "    for line in text:\n",
    "        values.append(line)\n",
    "        \n",
    "def get_values(names):\n",
    "    ret = []\n",
    "    for name in names:\n",
    "        suf = name.split('_')[1]\n",
    "        ret.append(int(suf.split('.')[0]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the text in the dir \n",
    "files = eachFile('aclImdbdemo/train/neg/')\n",
    "neg_v = get_values(files)\n",
    "neg_files = []\n",
    "for file in files:\n",
    "    neg_files.append(file)\n",
    "\n",
    "files = eachFile('aclImdbdemo/train/pos/')\n",
    "pos_v = get_values(files)\n",
    "pos_files = []\n",
    "for file in files:\n",
    "    pos_files.append(file)\n",
    "    \n",
    "files = eachFile('aclImdbdemo/train/unsup/')\n",
    "unsup_v = get_values(files)\n",
    "unsup_files = []\n",
    "for file in files:\n",
    "    unsup_files.append(file)\n",
    "        \n",
    "neg_text = []\n",
    "pos_text = []\n",
    "unsup_text = []\n",
    "for file in neg_files:\n",
    "    get_text(neg_text, file)\n",
    "for file in pos_files:\n",
    "    get_text(pos_text, file)\n",
    "for file in unsup_files:\n",
    "    get_text(unsup_text, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the text in the dir \n",
    "files = eachFile('aclImdbdemo/test/neg/')\n",
    "negt_v = get_values(files)\n",
    "negt_files = []\n",
    "for file in files:\n",
    "    negt_files.append(file)\n",
    "\n",
    "files = eachFile('aclImdbdemo/test/pos/')\n",
    "post_v = get_values(files)\n",
    "post_files = []\n",
    "for file in files:\n",
    "    post_files.append(file)\n",
    "    \n",
    "        \n",
    "neg_test = []\n",
    "pos_test = []\n",
    "for file in negt_files:\n",
    "    get_text(neg_test, file)\n",
    "for file in post_files:\n",
    "    get_text(pos_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# count all the words in the text\n",
    "import re,collections  \n",
    "def get_words(file):   \n",
    "    words_box=[]  \n",
    "    for line in file:\n",
    "        line = line.lower()\n",
    "        tokens = nltk.word_tokenize(line)\n",
    "        words_box.extend(tokens)                 \n",
    "    return collections.Counter(words_box) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_text = []\n",
    "for file in unsup_files:\n",
    "    get_text(unsup_text, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count =  get_words(neg_text)#Create a dictionary\n",
    "word_count = dict(word_count, ** get_words(pos_text))\n",
    "word_count = dict(word_count, ** get_words(unsup_text))\n",
    "word_count = dict(word_count, ** get_words(neg_test))#Create a dictionary\n",
    "word_count = dict(word_count, ** get_words(pos_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol='??,???,????,?????,??????,???????,????????,?????????,??????????,???????????,????????????,???????????????,!!,!!!,!!!!,!!!!!,!!!!!!,!!!!!!!,!!!!!!!!,!!!!!!!!!,!!!!!!!!!!,!!!!!!!!!!!,!!!!!!!!!!!!,?!!!!!!!!!!!!!!,!?,?!,!??,?!?,??!,!??!,((((((((((((((((((,xx,xxx,xxxx,xxxxx,:),:(,:D,:X,:x,:C,:c,:P,:p,:>,:<,:[,:],:|,:#,:o,:O,:/,:\\,;),;(,;D,;X,;x,;C,;c,;P,;p,;>,;<,;[,;],;|,;#,;o,;O,;/,;\\,;-),;-(,;-D,;-X,;-x,;-C,;-c,;-P,;-p,;->,;-<,;-[,;-],;-|,;-#,;-o,;-O,;-/,;-\\,:-),:-(,:-D,:-X,:-x,:-C,:-c,:-P,:-p,:->,:-<,:-[,:-],:-|,:-#,:-o,:-O,:-/,:-\\,:\\'(,((.,;_;,:-,???-??,!),(:,:**-(,XD,D;,):'\n",
    "symbols = symbol.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the category of the word by using nltk method\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "tags = set(['CC','DT','MD','IN','NN', 'NNP', 'NNS', 'NNPS','UH', 'VB','VBD', 'VBG','MD', 'VBN', 'VBP', 'VBZ', 'RB', 'RBR', 'RBS', 'JJ', 'JJR', 'JJS'])\n",
    "def filter(text):\n",
    "    # drop all the stop word and the word only show 1 time\n",
    "#     words = []\n",
    "    text = text.lower()\n",
    "#     pat_letter = re.compile(r'[^a-zA-Z \\']+')\n",
    "#     text = pat_letter.sub(' ', text).strip().lower()\n",
    "#     for w in text.split():\n",
    "#         if w in stopwords.words('english'):\n",
    "#             continue\n",
    "#         if w in symbols:\n",
    "#             words.append(w)\n",
    "#             continue\n",
    "#         if w is not None and w in word_count and word_count[w] > 1:\n",
    "#             words.append(w)\n",
    "            \n",
    "    words = [w for w in text.split() if((w in word_count and word_count[w] > 1) or w in symbols)]\n",
    "    # get the categoyr of word\n",
    "    pos_tags =nltk.pos_tag(words)\n",
    "    ret = []\n",
    "    #filter\n",
    "    for word,pos in pos_tags:\n",
    "        if (pos in tags):\n",
    "            ret.append(word)\n",
    "    return ' '.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the text after filtering\n",
    "neg_text = list(map(filter, neg_text))\n",
    "pos_text = list(map(filter, pos_text))\n",
    "unsup_text = list(map(filter, unsup_text))\n",
    "neg_test =  list(map(filter, neg_test))\n",
    "pos_test = list(map(filter,pos_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# structer the data\n",
    "neg_pd = pd.DataFrame(columns = ['content', 'label'])\n",
    "pos_pd = pd.DataFrame(columns = ['content', 'label'])\n",
    "unsup_pd = pd.DataFrame(columns = ['content', 'label'])\n",
    "negt_pd = pd.DataFrame(columns = ['content', 'label'])\n",
    "post_pd = pd.DataFrame(columns = ['content', 'label'])\n",
    "neg_pd['content'] = neg_text\n",
    "neg_pd['label'] = neg_v\n",
    "pos_pd['content'] = pos_text\n",
    "pos_pd['label'] = pos_v\n",
    "unsup_pd['content'] = unsup_text\n",
    "unsup_pd['label'] = unsup_v\n",
    "negt_pd['content'] = neg_test\n",
    "negt_pd['label'] = negt_v\n",
    "post_pd['content'] = pos_test\n",
    "post_pd['label'] = post_v\n",
    "data = pd.concat([pos_pd, neg_pd, unsup_pd, negt_pd, post_pd], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     10000\n",
       "10     2007\n",
       "1      1950\n",
       "4      1128\n",
       "8      1121\n",
       "3      1005\n",
       "9       978\n",
       "2       917\n",
       "7       894\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.label == 1, 'label'] = 2\n",
    "data.loc[data.label == 3, 'label'] = 4\n",
    "data.loc[data.label == 5, 'label'] = 6\n",
    "data.loc[data.label == 7, 'label'] = 8\n",
    "data.loc[data.label == 9, 'label'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(100)\n",
    "data.to_csv(\"scoreallafterfilter.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "count_vec = CountVectorizer()\n",
    "# cross validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.content, data.label, test_size=0.1, random_state=23)\n",
    "# word count by CountVectorizer\n",
    "x_train_mnb = count_vec.fit_transform(x_train)\n",
    "x_test_mnb = count_vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.55      0.61       978\n",
      "          2       0.47      0.52      0.49       289\n",
      "          4       0.24      0.50      0.33       204\n",
      "          8       0.22      0.24      0.23       199\n",
      "         10       0.48      0.35      0.40       330\n",
      "\n",
      "avg / total       0.52      0.48      0.49      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm  \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "#predict by SVM\n",
    "dtc = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "\n",
    "dtc.fit(x_train_mnb, y_train)\n",
    "print(classification_report(y_test, dtc.predict(x_test_mnb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
